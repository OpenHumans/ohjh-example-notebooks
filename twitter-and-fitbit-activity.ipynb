{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are daily step counts and the number of tweets sent per day correlated?\n",
    "They probably aren't but it's not unreasonable to think about it. After all, time spent on social media is time missing for being physically active and vice versa. Unless you're one of those people who walk around while tweeting (don't, it's dangerous!).\n",
    "\n",
    "![](http://www.kiss1023.ca/wp-content/uploads/sites/10/2017/07/textwalkpole.gif)\n",
    "\n",
    "But let's get started with our analysis. To do so we need to combine two data sources which you need to have in your Open Humans account:\n",
    "1. Your [Fitbit](https://www.openhumans.org/activity/fitbit-connection) account\n",
    "2. Your [Twitter archive](https://www.openhumans.org/activity/twitter-archive-analyzer/)\n",
    "\n",
    "With that out of the way we can start to load all of the Python dependencies we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pytz\n",
    "import io\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "import emoji\n",
    "\n",
    "\n",
    "# sets the axis label sizes for seaborn\n",
    "rc={'font.size': 14, 'axes.labelsize': 14, 'legend.fontsize': 14.0, \n",
    "    'axes.titlesize': 14, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Twitter archive data\n",
    "All the functions are already provided by http://twarxiv.org and we just copied the functions from there. It will basically take a zipped archive from a URL and return a nicely formatted `pandas` dataframe. Let's just take these as given for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE BELOW IS COPIED FROM TWARXIV.ORG AS IT ALREADY DOES EXACTLY WHAT WE WANT FOR READING IN THE DATA\n",
    "\n",
    "# READ JSON FILES FROM TWITTER ARCHIVE!\n",
    "\n",
    "def check_hashtag(single_tweet):\n",
    "    '''check whether tweet has any hashtags'''\n",
    "    return len(single_tweet['entities']['hashtags']) > 0\n",
    "\n",
    "\n",
    "def check_media(single_tweet):\n",
    "    '''check whether tweet has any media attached'''\n",
    "    return len(single_tweet['entities']['media']) > 0\n",
    "\n",
    "\n",
    "def check_url(single_tweet):\n",
    "    '''check whether tweet has any urls attached'''\n",
    "    return len(single_tweet['entities']['urls']) > 0\n",
    "\n",
    "\n",
    "def check_retweet(single_tweet):\n",
    "    '''\n",
    "    check whether tweet is a RT. If yes:\n",
    "    return name & user name of the RT'd user.\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'retweeted_status' in single_tweet.keys():\n",
    "        return (single_tweet['retweeted_status']['user']['screen_name'],\n",
    "                single_tweet['retweeted_status']['user']['name'])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def check_coordinates(single_tweet):\n",
    "    '''\n",
    "    check whether tweet has coordinates attached.\n",
    "    if yes return the coordinates\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'coordinates' in single_tweet['geo'].keys():\n",
    "        return (single_tweet['geo']['coordinates'][0],\n",
    "                single_tweet['geo']['coordinates'][1])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def check_reply_to(single_tweet):\n",
    "    '''\n",
    "    check whether tweet is a reply. If yes:\n",
    "    return name & user name of the user that's replied to.\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'in_reply_to_screen_name' in single_tweet.keys():\n",
    "        name = None\n",
    "        for user in single_tweet['entities']['user_mentions']:\n",
    "            if user['screen_name'] == single_tweet['in_reply_to_screen_name']:\n",
    "                name = user['name']\n",
    "                break\n",
    "        return (single_tweet['in_reply_to_screen_name'], name)\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def create_dataframe(tweets):\n",
    "    '''\n",
    "    create a pandas dataframe from our tweet jsons\n",
    "    '''\n",
    "\n",
    "    # initalize empty lists\n",
    "    utc_time = []\n",
    "    longitude = []\n",
    "    latitude = []\n",
    "    hashtag = []\n",
    "    media = []\n",
    "    url = []\n",
    "    retweet_user_name = []\n",
    "    retweet_name = []\n",
    "    reply_user_name = []\n",
    "    reply_name = []\n",
    "    text = []\n",
    "    # iterate over all tweets and extract data\n",
    "    for single_tweet in tweets:\n",
    "        utc_time.append(datetime.strptime(single_tweet['created_at'],\n",
    "                                                   '%Y-%m-%d %H:%M:%S %z'))\n",
    "        coordinates = check_coordinates(single_tweet)\n",
    "        latitude.append(coordinates[0])\n",
    "        longitude.append(coordinates[1])\n",
    "        hashtag.append(check_hashtag(single_tweet))\n",
    "        media.append(check_media(single_tweet))\n",
    "        url.append(check_url(single_tweet))\n",
    "        retweet = check_retweet(single_tweet)\n",
    "        retweet_user_name.append(retweet[0])\n",
    "        retweet_name.append(retweet[1])\n",
    "        reply = check_reply_to(single_tweet)\n",
    "        reply_user_name.append(reply[0])\n",
    "        reply_name.append(reply[1])\n",
    "        text.append(single_tweet['text'])\n",
    "    # convert the whole shebang into a pandas dataframe\n",
    "    dataframe = pd.DataFrame(data={\n",
    "                            'utc_time': utc_time,\n",
    "                            'latitude': latitude,\n",
    "                            'longitude': longitude,\n",
    "                            'hashtag': hashtag,\n",
    "                            'media': media,\n",
    "                            'url': url,\n",
    "                            'retweet_user_name': retweet_user_name,\n",
    "                            'retweet_name': retweet_name,\n",
    "                            'reply_user_name': reply_user_name,\n",
    "                            'reply_name': reply_name,\n",
    "                            'text': text\n",
    "    })\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def read_files(zip_url):\n",
    "    tf = tempfile.NamedTemporaryFile()\n",
    "    print('downloading files')\n",
    "    tf.write(requests.get(zip_url).content)\n",
    "    tf.flush()\n",
    "    zf = zipfile.ZipFile(tf.name)\n",
    "    print('reading index')\n",
    "    with zf.open('data/js/tweet_index.js', 'r') as f:\n",
    "        f = io.TextIOWrapper(f)\n",
    "        d = f.readlines()[1:]\n",
    "        d = \"[{\" + \"\".join(d)\n",
    "        json_files = json.loads(d)\n",
    "    data_frames = []\n",
    "    print('iterate over individual files')\n",
    "    for single_file in json_files:\n",
    "        print('read ' + single_file['file_name'])\n",
    "        with zf.open(single_file['file_name']) as f:\n",
    "            f = io.TextIOWrapper(f)\n",
    "            d = f.readlines()[1:]\n",
    "            d = \"\".join(d)\n",
    "            tweets = json.loads(d)\n",
    "            df_tweets = create_dataframe(tweets)\n",
    "            data_frames.append(df_tweets)\n",
    "    return data_frames\n",
    "\n",
    "\n",
    "def create_main_dataframe(zip_url='http://ruleofthirds.de/test_archive.zip'):\n",
    "    print('reading files')\n",
    "    dataframes = read_files(zip_url)\n",
    "    print('concatenating...')\n",
    "    dataframe = pd.concat(dataframes)\n",
    "    dataframe = dataframe.sort_values('utc_time', ascending=False)\n",
    "    dataframe = dataframe.set_index('utc_time')\n",
    "    dataframe = dataframe.replace(to_replace={\n",
    "                                    'url': {False: None},\n",
    "                                    'hashtag': {False: None},\n",
    "                                    'media': {False: None}\n",
    "                                    })\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading & importing Fitbit/Twitter data\n",
    "Now that we have set up all the functions we need for importing the Twitter data, let's grab the data itself. We're getting our data URLs from Open Humans and import both Twitter & Fitbit data (this might take a while if you have lots of tweets/fitbit data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files\n",
      "downloading files\n",
      "reading index\n",
      "iterate over individual files\n",
      "read data/js/tweets/2018_03.js\n",
      "read data/js/tweets/2018_02.js\n",
      "read data/js/tweets/2018_01.js\n",
      "read data/js/tweets/2017_12.js\n",
      "read data/js/tweets/2017_11.js\n",
      "read data/js/tweets/2017_10.js\n",
      "read data/js/tweets/2017_09.js\n",
      "read data/js/tweets/2017_08.js\n",
      "read data/js/tweets/2017_07.js\n",
      "read data/js/tweets/2017_06.js\n",
      "read data/js/tweets/2017_05.js\n",
      "read data/js/tweets/2017_04.js\n",
      "read data/js/tweets/2017_03.js\n",
      "read data/js/tweets/2017_02.js\n",
      "read data/js/tweets/2017_01.js\n",
      "read data/js/tweets/2016_12.js\n",
      "read data/js/tweets/2016_11.js\n",
      "read data/js/tweets/2016_10.js\n",
      "read data/js/tweets/2016_09.js\n",
      "read data/js/tweets/2016_08.js\n",
      "read data/js/tweets/2016_07.js\n",
      "read data/js/tweets/2016_06.js\n",
      "read data/js/tweets/2016_05.js\n",
      "read data/js/tweets/2016_04.js\n",
      "read data/js/tweets/2016_03.js\n",
      "read data/js/tweets/2016_02.js\n",
      "read data/js/tweets/2016_01.js\n",
      "read data/js/tweets/2015_12.js\n",
      "read data/js/tweets/2015_11.js\n",
      "read data/js/tweets/2015_10.js\n",
      "read data/js/tweets/2015_09.js\n",
      "read data/js/tweets/2015_08.js\n",
      "read data/js/tweets/2015_07.js\n",
      "read data/js/tweets/2015_06.js\n",
      "read data/js/tweets/2015_05.js\n",
      "read data/js/tweets/2015_04.js\n",
      "read data/js/tweets/2015_03.js\n",
      "read data/js/tweets/2015_02.js\n",
      "read data/js/tweets/2015_01.js\n",
      "read data/js/tweets/2014_12.js\n",
      "read data/js/tweets/2014_11.js\n",
      "read data/js/tweets/2014_10.js\n",
      "read data/js/tweets/2014_09.js\n",
      "read data/js/tweets/2014_08.js\n",
      "read data/js/tweets/2014_07.js\n",
      "read data/js/tweets/2014_06.js\n",
      "read data/js/tweets/2014_05.js\n",
      "read data/js/tweets/2014_04.js\n",
      "read data/js/tweets/2014_03.js\n",
      "read data/js/tweets/2014_02.js\n",
      "read data/js/tweets/2014_01.js\n",
      "read data/js/tweets/2013_12.js\n",
      "read data/js/tweets/2013_11.js\n",
      "read data/js/tweets/2013_10.js\n",
      "read data/js/tweets/2013_09.js\n",
      "read data/js/tweets/2013_08.js\n",
      "read data/js/tweets/2013_07.js\n",
      "read data/js/tweets/2013_06.js\n",
      "read data/js/tweets/2013_05.js\n",
      "read data/js/tweets/2013_04.js\n",
      "read data/js/tweets/2013_03.js\n",
      "read data/js/tweets/2013_02.js\n",
      "read data/js/tweets/2013_01.js\n",
      "read data/js/tweets/2012_12.js\n",
      "read data/js/tweets/2012_11.js\n",
      "read data/js/tweets/2012_10.js\n",
      "read data/js/tweets/2012_09.js\n",
      "read data/js/tweets/2012_08.js\n",
      "read data/js/tweets/2012_07.js\n",
      "read data/js/tweets/2012_06.js\n",
      "read data/js/tweets/2012_05.js\n",
      "read data/js/tweets/2012_04.js\n",
      "read data/js/tweets/2012_03.js\n",
      "read data/js/tweets/2012_02.js\n",
      "read data/js/tweets/2012_01.js\n",
      "read data/js/tweets/2011_12.js\n",
      "read data/js/tweets/2011_11.js\n",
      "read data/js/tweets/2011_10.js\n",
      "read data/js/tweets/2011_09.js\n",
      "read data/js/tweets/2011_08.js\n",
      "read data/js/tweets/2011_07.js\n",
      "read data/js/tweets/2011_06.js\n",
      "read data/js/tweets/2011_05.js\n",
      "read data/js/tweets/2011_04.js\n",
      "read data/js/tweets/2011_03.js\n",
      "read data/js/tweets/2011_02.js\n",
      "read data/js/tweets/2011_01.js\n",
      "read data/js/tweets/2010_12.js\n",
      "read data/js/tweets/2010_11.js\n",
      "read data/js/tweets/2010_10.js\n",
      "read data/js/tweets/2010_09.js\n",
      "read data/js/tweets/2010_08.js\n",
      "read data/js/tweets/2010_07.js\n",
      "read data/js/tweets/2010_06.js\n",
      "read data/js/tweets/2010_05.js\n",
      "read data/js/tweets/2010_04.js\n",
      "read data/js/tweets/2010_03.js\n",
      "read data/js/tweets/2010_02.js\n",
      "read data/js/tweets/2010_01.js\n",
      "read data/js/tweets/2009_12.js\n",
      "read data/js/tweets/2009_11.js\n",
      "read data/js/tweets/2009_10.js\n",
      "read data/js/tweets/2009_09.js\n",
      "read data/js/tweets/2009_08.js\n",
      "read data/js/tweets/2009_07.js\n",
      "read data/js/tweets/2009_06.js\n",
      "read data/js/tweets/2009_05.js\n",
      "read data/js/tweets/2009_04.js\n",
      "read data/js/tweets/2009_03.js\n",
      "read data/js/tweets/2009_02.js\n",
      "read data/js/tweets/2009_01.js\n",
      "read data/js/tweets/2008_12.js\n",
      "read data/js/tweets/2008_11.js\n",
      "read data/js/tweets/2008_10.js\n",
      "read data/js/tweets/2008_09.js\n",
      "read data/js/tweets/2008_08.js\n",
      "read data/js/tweets/2008_07.js\n",
      "read data/js/tweets/2008_06.js\n",
      "read data/js/tweets/2008_05.js\n",
      "read data/js/tweets/2008_04.js\n",
      "concatenating...\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://www.openhumans.org/api/direct-sharing/project/exchange-member/?access_token={}\".format(os.environ.get('OH_ACCESS_TOKEN')))\n",
    "user = json.loads(response.content)\n",
    "\n",
    "has_twitter = False\n",
    "has_fitbit = False\n",
    "\n",
    "# get our download URLs\n",
    "for entry in user['data']:\n",
    "    if entry['source'] == \"direct-sharing-70\":\n",
    "        twitter_data_url = entry['download_url']\n",
    "        has_twitter = True\n",
    "    if entry['source'] == \"direct-sharing-102\":\n",
    "        fitbit_data_url = entry['download_url']\n",
    "        has_moves = True\n",
    "\n",
    "if not has_twitter:\n",
    "    print(\"YOU NEED TO HAVE SOME TWITTER DATA IN YOUR ACCOUNT TO USE THIS NOTEBOOK\")\n",
    "    print(\"GO TO http://twarxiv.org TO UPLOAD IT\")\n",
    "\n",
    "if not has_twitter:\n",
    "    print(\"YOU NEED TO HAVE SOME FITBIT DATA IN YOUR ACCOUNT TO USE THIS NOTEBOOK\")\n",
    "    print(\"GO TO https://ohmovessource.herokuapp.com/ TO UPLOAD IT\")\n",
    "    \n",
    "# read the twitter data\n",
    "twitter_data = create_main_dataframe(zip_url=twitter_data_url)\n",
    "\n",
    "# read the fitbit data\n",
    "fitbit_data_raw = requests.get(fitbit_data_url).content\n",
    "fitbit_data = json.loads(fitbit_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our raw *Fitbit* data into a `pandas` dataframe - this will make it much easier to plot our data and unite it with the `Twitter` data at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "steps = []\n",
    "\n",
    "for year in fitbit_data['tracker-steps'].keys():\n",
    "    for entry in fitbit_data['tracker-steps'][year]['activities-tracker-steps']:\n",
    "        date.append(entry['dateTime'])\n",
    "        steps.append(entry['value'])\n",
    "        \n",
    "fitbit_steps = pd.DataFrame(data={\n",
    "                'date':date,\n",
    "                'steps': steps})\n",
    "fitbit_steps['date'] = pd.to_datetime(fitbit_steps['date'])\n",
    "fitbit_steps = fitbit_steps.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the daily `steps` along with the `date`, nicely formatted. We can now prepare our twitter-data and calculate the number of `tweets_per_day` for our final plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['date'] = twitter_data.index.date\n",
    "tweets_per_day = twitter_data['date'].value_counts()\n",
    "twitter_mean = twitter_data.groupby(twitter_data.index.date).mean()\n",
    "twitter_mean = twitter_mean.join(tweets_per_day)\n",
    "twitter_mean['tweets_per_day'] = twitter_mean['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way we can join our data and join our two `twitter_mean` and `fitbit_steps` dataframes into one. To make sure to not bias our data analysis for dates during which we didn't collect any data we remove all days during which we didn't record `steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = twitter_mean.join(fitbit_steps,how='right')\n",
    "joined_data['steps'] = joined_data['steps'].apply(int)\n",
    "joined_data = joined_data[joined_data['steps'] != 0]\n",
    "joined_data['year'] = joined_data.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>media</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>tweets_per_day</th>\n",
       "      <th>steps</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10978</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11639</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14943</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16222</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14165</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hashtag  media  url  date  tweets_per_day  steps  year\n",
       "date                                                              \n",
       "2013-01-01      1.0    NaN  1.0  19.0            19.0  10978  2013\n",
       "2013-01-02      NaN    1.0  1.0  43.0            43.0  11639  2013\n",
       "2013-01-03      NaN    NaN  1.0  25.0            25.0  14943  2013\n",
       "2013-01-04      NaN    NaN  1.0  18.0            18.0  16222  2013\n",
       "2013-01-05      1.0    1.0  1.0  24.0            24.0  14165  2013"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our daily step counts and number of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"steps\", \n",
    "           y=\"tweets_per_day\", \n",
    "           data=joined_data[joined_data.index.year == 2014],\n",
    "           lowess=True,\n",
    "           size=8,\n",
    "           aspect=1.5,\n",
    "           scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is some correlation between the number of daily steps and the number of daily tweets: On days with more tweets there are less tweets. But this might be a result of tweeting more/less over time in general or walking more/less over time. So let's break it down into yearly numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"steps\", y=\"tweets_per_day\", row='year', data=joined_data,lowess=True,size=2,aspect=2,scatter_kws={'alpha':0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks different. First of all we see that the number of daily tweets drops a lot from `2012` to `2018`. And additionally we see that the number of daily steps is a lot lower for `2016` and `2017`. \n",
    "\n",
    "Which makes sense: \n",
    "1. For much of 2015/2016 I didn't wear any Fitbit but rather only used the iPhone app to import data.\n",
    "2. I also happened to write my PhD thesis during those years, so I moved and tweeted less. 😂\n",
    "\n",
    "Does your Twitter activity and physical activity correlate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
